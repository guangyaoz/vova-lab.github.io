<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://guangyaoz.github.io/vova-lab.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://guangyaoz.github.io/vova-lab.github.io/" rel="alternate" type="text/html" /><updated>2024-12-30T19:08:20+00:00</updated><id>https://guangyaoz.github.io/vova-lab.github.io/feed.xml</id><title type="html">random lab site</title><subtitle>An engaging 1-3 sentence description of your lab.</subtitle><entry><title type="html">Asynchronous Decentralized Federated Lifelong Learning (ADFLL)</title><link href="https://guangyaoz.github.io/vova-lab.github.io/2024/12/30/ADFLL.html" rel="alternate" type="text/html" title="Asynchronous Decentralized Federated Lifelong Learning (ADFLL)" /><published>2024-12-30T00:00:00+00:00</published><updated>2024-12-30T19:07:22+00:00</updated><id>https://guangyaoz.github.io/vova-lab.github.io/2024/12/30/ADFLL</id><content type="html" xml:base="https://guangyaoz.github.io/vova-lab.github.io/2024/12/30/ADFLL.html"><![CDATA[<p>This project introduces the Asynchronous Decentralized Federated Lifelong Learning (ADFLL) framework, an innovative approach to federated learning that addresses the limitations of synchronous training schedules and the lack of lifelong learning in conventional machine learning frameworks for medical applications. ADFLL enables asynchronous and continual learning across agents, allowing them to leverage both their own experiences and knowledge shared by others. The framework was evaluated using deep reinforcement learning (DRL) for landmark localization tasks across diverse imaging modalities, orientations, and sequences. Experimental results demonstrated that ADFLL outperforms baseline models in collaborative learning, showing superior performance on both in-distribution and out-of-distribution test sets. This robust, efficient, and flexible framework is well-suited for deployment in real-world applications requiring privacy-preserving and lifelong collaborative learning. Paper published in <em>Medical Imaging with Deep Learning</em> (MIDL) 2024, <a href="https://openreview.net/forum?id=FbM7sDDAZ4">Towards a Collective Medical Imaging AI: Enabling Continual Learning from Peers</a>. Github repo: https://github.com/guangyaoz/ADFLL</p>

<p><img src="/images/ADFLL_Concept_Figure.png" alt="plain image" /></p>

<p>To further enhance the efficiency of the ADFLL framework, we integrate a reward distribution-preserving coreset compression technique for selective experience replay buffers. This method compresses stored experiences, reducing computational overhead while maintaining the model’s ability to mitigate catastrophic forgetting. Evaluated on tasks such as ventricle localization in the BRATS dataset and landmark localization in whole-body MRI, the compressed lifelong learning models demonstrated competitive performance with minimal impact on accuracy. For instance, the 10x compressed models achieved mean pixel distances close to conventional lifelong learning models, highlighting the viability of coreset compression in resource-constrained settings. This addition reinforces ADFLL’s scalability and adaptability to real-world applications. Paper published in <em>Medical Imaging with Deep Learning</em> (MIDL) 2023, <a href="https://proceedings.mlr.press/v227/zheng24a.html">Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging</a>.</p>

<p>To further address the challenges of deploying the ADFLL framework on low-compute edge devices in rapidly evolving imaging environments, we developed three image coreset algorithms for compressing and denoising medical images in selective experience replay. These include neighborhood averaging, neighborhood sensitivity-based sampling, and maximum entropy coresets, which achieve 27x compression while maintaining strong performance in localizing anatomical landmarks on DIXON water and fat MRI images. Notably, the maximum entropy coreset outperformed conventional lifelong learning models with an average distance error of 11.97±12.02 compared to 19.24±50.77, showcasing its potential to enhance efficiency and adaptability in real-world medical imaging applications. Paper on arXiv: <a href="https://arxiv.org/abs/2306.05310">A framework for dynamically training and adapting deep reinforcement learning models to different, low-compute, and continuously changing radiology deployment environments</a></p>

<p>This project is funded by DARPA, part of the Shared Experience Lifelong Learning (<a href="https://intelligencecommunitynews.com/darpa-launches-shell-program/">ShELL</a>) program. Collectively, we have published a paper on <em>Nature Machine Intelligence</em>, <a href="https://rdcu.be/dB9zt">A collective AI via lifelong learning and sharing at the edge</a></p>]]></content><author><name>Guangyao Zheng</name></author><category term="Distributed Learning" /><category term="Continual Learning" /><category term="Medical Imaging" /><category term="Low-compute Optimization" /><summary type="html"><![CDATA[This project introduces the Asynchronous Decentralized Federated Lifelong Learning (ADFLL) framework, an innovative approach to federated learning that addresses the limitations of synchronous training schedules and the lack of lifelong learning in conventional machine learning frameworks for medical applications. ADFLL enables asynchronous and continual learning across agents, allowing them to leverage both their own experiences and knowledge shared by others. The framework was evaluated using deep reinforcement learning (DRL) for landmark localization tasks across diverse imaging modalities, orientations, and sequences. Experimental results demonstrated that ADFLL outperforms baseline models in collaborative learning, showing superior performance on both in-distribution and out-of-distribution test sets. This robust, efficient, and flexible framework is well-suited for deployment in real-world applications requiring privacy-preserving and lifelong collaborative learning. Paper published in Medical Imaging with Deep Learning (MIDL) 2024, Towards a Collective Medical Imaging AI: Enabling Continual Learning from Peers. Github repo: https://github.com/guangyaoz/ADFLL]]></summary></entry><entry><title type="html">Example post 3</title><link href="https://guangyaoz.github.io/vova-lab.github.io/2023/02/23/example-post-3.html" rel="alternate" type="text/html" title="Example post 3" /><published>2023-02-23T00:00:00+00:00</published><updated>2024-12-30T19:07:22+00:00</updated><id>https://guangyaoz.github.io/vova-lab.github.io/2023/02/23/example-post-3</id><content type="html" xml:base="https://guangyaoz.github.io/vova-lab.github.io/2023/02/23/example-post-3.html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>]]></content><author><name>john-doe</name></author><category term="biology," /><category term="medicine" /><summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://guangyaoz.github.io/vova-lab.github.io/images/photo.jpg" /><media:content medium="image" url="https://guangyaoz.github.io/vova-lab.github.io/images/photo.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Example post 2</title><link href="https://guangyaoz.github.io/vova-lab.github.io/2021/09/30/example-post-2.html" rel="alternate" type="text/html" title="Example post 2" /><published>2021-09-30T00:00:00+00:00</published><updated>2024-12-30T19:07:22+00:00</updated><id>https://guangyaoz.github.io/vova-lab.github.io/2021/09/30/example-post-2</id><content type="html" xml:base="https://guangyaoz.github.io/vova-lab.github.io/2021/09/30/example-post-2.html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>]]></content><author><name>jane-smith</name></author><summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.]]></summary></entry><entry><title type="html">Example post 1</title><link href="https://guangyaoz.github.io/vova-lab.github.io/2019/01/07/example-post-1.html" rel="alternate" type="text/html" title="Example post 1" /><published>2019-01-07T00:00:00+00:00</published><updated>2024-12-30T19:07:22+00:00</updated><id>https://guangyaoz.github.io/vova-lab.github.io/2019/01/07/example-post-1</id><content type="html" xml:base="https://guangyaoz.github.io/vova-lab.github.io/2019/01/07/example-post-1.html"><![CDATA[<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.</p>]]></content><author><name>sarah-johnson</name></author><category term="biology" /><category term="medicine" /><category term="big data" /><summary type="html"><![CDATA[Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.]]></summary></entry></feed>